{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-z_TsM8qA1hz"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "from parse_config import parse_model_config\n",
    "from models import Darknet\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JWqLJ3J9DHAS"
   },
   "outputs": [],
   "source": [
    "LABELLED_DATA_PATH = 'data/train_images/'\n",
    "\n",
    "def img_transform(img_path, boxes):\n",
    "    img = cv2.imread(img_path)\n",
    "    h, w = img.shape[:2]\n",
    "    square_dim = max(w,h)\n",
    "    pad_len = abs(w-h)//2\n",
    "    pad_dim = (0,0, 0,0, pad_len, pad_len) if w >= h else (0,0, pad_len,pad_len, 0,0)\n",
    "    img = F.pad(torch.from_numpy(img), pad_dim, )\n",
    "    shift_x, shift_y = (0, pad_len) if w>=h else (pad_len, 0)\n",
    "    boxes = torch.IntTensor(boxes)\n",
    "    boxes[..., :2] += torch.IntTensor([shift_x, shift_y])\n",
    "    boxes[:, :2] += (boxes[:, 2:]//2)\n",
    "    boxes = boxes/float(img.shape[0])\n",
    "    img = F.interpolate(img.permute(2,0,1).unsqueeze(0), size=416, mode='nearest').float().squeeze(0)\n",
    "    return img, boxes\n",
    "\n",
    "def draw_boxes(img, box_norm):\n",
    "    h, w = img.shape[:2]\n",
    "    square_dim = max(w,h)\n",
    "    pad_len = abs(w-h)//2\n",
    "    boxes = (box_norm * w).astype(int)\n",
    "    shift_x, shift_y = (0, pad_len) if w>=h else (pad_len, 0)\n",
    "    print(pad_len)\n",
    "    boxes[:, :2] -= [shift_x, shift_y]\n",
    "    boxes[:, :2] -= (boxes[:, 2:]//2)\n",
    "    for x1, y1, w, h in boxes:\n",
    "        cv2.rectangle(img, (x1, y1), (x1+w, y1+h), (255, 0, 0), 2)\n",
    "\n",
    "    plt.imshow(img)\n",
    "\n",
    "class FaceTrainSet(Dataset):\n",
    "    def __init__(self, data_path, labels_path, transforms=None):\n",
    "        super(FaceTrainSet, self).__init__()\n",
    "        self.data_path = data_path\n",
    "        self.transforms = transforms\n",
    "        self.labels_path = labels_path\n",
    "\n",
    "        self.labels = np.loadtxt(self.labels_path, dtype=str, delimiter='\\n')\n",
    "        self.file_names = list(filter(lambda x:x.endswith('.jpg'), self.labels))\n",
    "        self.file_name_index = [i for i in range(len(self.labels)) if self.labels[i].endswith('.jpg')]\n",
    "        \n",
    "        self.batch_count = 0\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        idx = self.file_name_index[index]\n",
    "        try:\n",
    "            label_info = self.labels[idx: self.file_name_index[self.file_name_index.index(idx)+1]]\n",
    "        except IndexError as e:\n",
    "            label_info = self.labels[idx:]\n",
    "        file_name = label_info[0]\n",
    "        print(file_name)\n",
    "        num_faces = label_info[1]\n",
    "        box_dims = [np.array(values.split()).astype(np.int)[:4] for values in label_info[2:]]\n",
    "        img_tensor, boxes = img_transform(LABELLED_DATA_PATH+file_name, box_dims)\n",
    "        return (img_tensor, boxes, num_faces)\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        img_tensors, targets, num_faces = list(zip(*batch))\n",
    "        # Remove empty placeholder targets\n",
    "        targets = [boxes for boxes in targets if boxes is not None] \n",
    "        # Add sample index to targets\n",
    "        for i, boxes in enumerate(targets):\n",
    "            index = torch.ones(len(boxes), 1) * i\n",
    "            targets[i] = torch.cat((index, boxes), 1)\n",
    "        targets = torch.cat(targets, 0)\n",
    "        # Selects new image size every tenth batch\n",
    "        #         if self.multiscale and self.batch_count % 10 == 0:\n",
    "        #             self.img_size = random.choice(range(self.min_size, self.max_size + 1, 32))\n",
    "        #         # Resize images to input shape\n",
    "        #         imgs = torch.stack([resize(img, self.img_size) for img in imgs])\n",
    "        img_tensors = torch.stack([img for img in img_tensors])\n",
    "        self.batch_count += 1\n",
    "        return img_tensors, targets, num_faces\n",
    "    \n",
    "def xywh2xyxy(x):\n",
    "    y = x.new(x.shape)\n",
    "    y[..., 0] = x[..., 0] - x[..., 2] // 2\n",
    "    y[..., 1] = x[..., 1] - x[..., 3] // 2\n",
    "    y[..., 2] = x[..., 0] + x[..., 2] // 2\n",
    "    y[..., 3] = x[..., 1] + x[..., 3] // 2\n",
    "    return y\n",
    "\n",
    "def non_max_suppression(prediction, conf_thres=0.5, nms_thres=0.4):\n",
    "    \"\"\"\n",
    "    Removes detections with lower object confidence score than 'conf_thres' and performs\n",
    "    Non-Maximum Suppression to further filter detections.\n",
    "    Returns detections with shape:\n",
    "        (x1, y1, x2, y2, object_conf, class_score, class_pred)\n",
    "    \"\"\"\n",
    "\n",
    "    # From (center x, center y, width, height) to (x1, y1, x2, y2)\n",
    "    prediction[..., :4] = xywh2xyxy(prediction[..., :4])\n",
    "    output = [None for _ in range(len(prediction))]\n",
    "    for image_i, image_pred in enumerate(prediction):\n",
    "        # Filter out confidence scores below threshold\n",
    "        image_pred = image_pred[image_pred[:, 4] >= conf_thres]\n",
    "        # If none are remaining => process next image\n",
    "        if not image_pred.size(0):\n",
    "            continue\n",
    "        # Object confidence times class confidence\n",
    "        print(image_pred[:, 4].shape, image_pred[:, 5:].shape)\n",
    "        score = image_pred[:, 4] * image_pred[:, 5:].max(1)[0]\n",
    "        # Sort by it\n",
    "        image_pred = image_pred[(-score).argsort()]\n",
    "        class_confs, class_preds = image_pred[:, 5:].max(1, keepdim=True)\n",
    "        detections = torch.cat((image_pred[:, :5], class_confs.float(), class_preds.float()), 1)\n",
    "        # Perform non-maximum suppression\n",
    "        keep_boxes = []\n",
    "        while detections.size(0):\n",
    "            large_overlap = bbox_iou(detections[0, :4].unsqueeze(0), detections[:, :4]) > nms_thres\n",
    "            label_match = detections[0, -1] == detections[:, -1]\n",
    "            # Indices of boxes with lower confidence scores, large IOUs and matching labels\n",
    "            invalid = large_overlap & label_match\n",
    "            weights = detections[invalid, 4:5]\n",
    "            # Merge overlapping bboxes by order of confidence\n",
    "            detections[0, :4] = (weights * detections[invalid, :4]).sum(0) / weights.sum()\n",
    "            keep_boxes += [detections[0]]\n",
    "            detections = detections[~invalid]\n",
    "        if keep_boxes:\n",
    "            output[image_i] = torch.stack(keep_boxes)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RKbcXO3t-wDh"
   },
   "outputs": [],
   "source": [
    "dataset = FaceTrainSet('data/train_images/', 'data/labels.txt')\n",
    "dataloader = DataLoader(dataset, batch_size=2, collate_fn = dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58--Hockey/58_Hockey_icehockey_puck_58_399.jpg\n",
      "58--Hockey/58_Hockey_icehockey_puck_58_286.jpg\n"
     ]
    }
   ],
   "source": [
    "img_tensors, boxes, num_faces = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Darknet('yolov3.cfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "out= model(img_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10647, 5])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#non_max_suppression(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_1 = out[0]\n",
    "out_2 = out[1]\n",
    "out_3 = out[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_boxes = out_1[..., :4]\n",
    "pred_boxes.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_wh_iou(anch_x, anch_y, target_x, target_y):\n",
    "    inter_area = torch.min(target_x, anch_x) * torch.min(target_y, anch_y)\n",
    "    union_area = (anch_x * anch_y) + (target_x * target_y) - inter_area\n",
    "    return inter_area/union_area\n",
    "\n",
    "def build_targets(pred_boxes, target, anchors, ignore_thres):\n",
    "    ByteTensor = torch.cuda.ByteTensor if pred_boxes.is_cuda else torch.ByteTensor\n",
    "    FloatTensor = torch.cuda.FloatTensor if pred_boxes.is_cuda else torch.FloatTensor\n",
    "\n",
    "    nS = pred_boxes.size(0)\n",
    "    nA = pred_boxes.size(1)\n",
    "    nG = pred_boxes.size(2)\n",
    "\n",
    "    # Output tensors\n",
    "    obj_mask = ByteTensor(nS, nA, nG, nG).fill_(0)\n",
    "    noobj_mask = ByteTensor(nS, nA, nG, nG).fill_(1)\n",
    "    iou_scores = FloatTensor(nS, nA, nG, nG).fill_(0)\n",
    "    tx = FloatTensor(nS, nA, nG, nG).fill_(0)\n",
    "    ty = FloatTensor(nS, nA, nG, nG).fill_(0)\n",
    "    tw = FloatTensor(nS, nA, nG, nG).fill_(0)\n",
    "    th = FloatTensor(nS, nA, nG, nG).fill_(0)\n",
    "    target_boxes = target[:, 1:5]\n",
    "    s = target[:, 0].long()\n",
    "    g_w, g_h = target_boxes[:, 2:4].T\n",
    "    g_xy = target_boxes[:, 0:2] * nG\n",
    "    g_i, g_j = g_xy.long().t()\n",
    "    g_x, g_y = g_xy.t()\n",
    "    \n",
    "    ious = torch.stack([bbox_wh_iou(anch_x, anch_y, g_w, g_h) for anch_x, anch_y in anchors])\n",
    "    best_ious, ind = ious.max(0)\n",
    "    \n",
    "    obj_mask[s, ind, g_i, g_j] = 1\n",
    "    noobj_mask[s, ind, g_i, g_j] = 0\n",
    "    \n",
    "    tx[s, ind, g_i, g_j] = g_x - g_x.floor()\n",
    "    ty[s, ind, g_i, g_j] = g_y - g_y.floor()\n",
    "    \n",
    "    for i, anchor_ious in enumerate(ious.t()):\n",
    "        noobj_mask[s[i], anchor_ious > ignore_thres, g_i[i], g_j[i]] = 0\n",
    "    \n",
    "    tw[s, ind, g_i, g_j] = torch.log(g_w/anchors[ind][:, 0] + 1e-16)\n",
    "    th[s, ind, g_i, g_j] = torch.log(g_h/anchors[ind][:, 1] + 1e-16)\n",
    "    \n",
    "    iou_scores[s, ind, g_i, g_j] = bbox_iou(pred_boxes[s, ind, g_i, g_j], target_boxes,  x1y1x2y2=False)\n",
    "    \n",
    "    tconf = obj_mask.float()\n",
    "    return iou_scores, obj_mask, noobj_mask, tx, ty, tw, th, tconf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors = model.module_defs[-1]['anchors'].split(',')\n",
    "anchors = [int(x) for x in anchors]\n",
    "anchors = [(anchors[i]/416/13, anchors[i+1]/416/13) for i in range(0, len(anchors), 2)]\n",
    "anchors = torch.FloatTensor(anchors[:3])\n",
    "\n",
    "iou_scores = torch.FloatTensor(2, 3, 13, 13).fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_boxes = ((boxes[:, 1:5])) * 13\n",
    "s = boxes[:, 0].long()\n",
    "target_xy = target_boxes[:, 0:2]\n",
    "target_wh = target_boxes[:, 2:4]\n",
    "target_x, target_y = target_xy.long().t()\n",
    "target_w, target_h = target_wh.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_wh_iou(anch_x, anch_y, target_x, target_y):\n",
    "    inter_area = torch.min(target_x, anch_x) * torch.min(target_y, anch_y)\n",
    "    union_area = (anch_x * anch_y) + (target_x * target_y) - inter_area\n",
    "    return inter_area/union_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ious = torch.stack([bbox_wh_iou(anch_x, anch_y, target_w, target_h) for anch_x, anch_y in anchors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ious, ind = ious.max(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_iou(boxes1, boxes2, x1y1x2y2=False):\n",
    "    if not x1y1x2y2:\n",
    "        boxes1_x1, boxes1_x2 = boxes1[:,0] - boxes1[:,2]/2, boxes1[:,0] + boxes1[:,2]/2\n",
    "        boxes1_y1, boxes1_y2 = boxes1[:,1] - boxes1[:,3]/2, boxes1[:,1]+boxes1[:,3]/2\n",
    "        boxes2_x1, boxes2_x2 = boxes2[:,0] - boxes2[:,2]/2, boxes2[:,0] + boxes2[:,2]/2\n",
    "        boxes2_y1, boxes2_y2 = boxes2[:,1] - boxes2[:,3]/2, boxes2[:,1]+boxes2[:,3]/2\n",
    "    \n",
    "    else:\n",
    "        boxes1_x1, boxes1_x2, boxes1_y1, boxes1_y2 = boxes1[:, 0], boxes1[:, 1], boxes1[:, 2], boxes1[:, 3]\n",
    "        boxes2_x1, boxes2_x2, boxes2_y1, boxes2_y2 = boxes2[:, 0], boxes2[:, 1], boxes2[:, 2], boxes2[:, 3]\n",
    "        \n",
    "    inter_w =  torch.clamp(torch.min(boxes1_x2, boxes2_x2) - torch.max(boxes1_x1, boxes2_x1) + 1, min=0)\n",
    "    inter_h = torch.clamp(torch.min(boxes1_y2, boxes2_y2) - torch.max(boxes1_y1, boxes2_y1 ) + 1, min=0)\n",
    "    \n",
    "    inter_area = inter_w * inter_h\n",
    "    \n",
    "    boxes1_area = (boxes1_x2-boxes1_x1+1) * (boxes1_y2-boxes1_y1+1)\n",
    "    boxes2_area = (boxes2_x2-boxes2_x1+1) * (boxes2_y2-boxes2_y1+1)\n",
    "    \n",
    "    iou = inter_area/(boxes1_area + boxes2_area - inter_area+1e-16)\n",
    "    \n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes1 = boxes[:, 1:]\n",
    "boxes2 = torch.cat([boxes1, boxes1+0.01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('data/train_images/58--Hockey/58_Hockey_icehockey_puck_58_399.jpg')\n",
    "draw_boxes(img, boxes2.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    with open ('config.yml', 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "from models import *\n",
    "#from utils.logger import *\n",
    "#from utils.utils import *\n",
    "from utils import *\n",
    "from data_manager import *\n",
    "from parse_config import *\n",
    "#from test import evaluate\n",
    "\n",
    "from terminaltables import AsciiTable\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import argparse\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"--config\", type=str, default='config.yml', help=\"configuration file in yml format\")\n",
    "# opt = parser.parse_args()\n",
    "\n",
    "#config_file = opt.config\n",
    "try: \n",
    "    with open ('config.yml', 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "except Exception as e:\n",
    "    print(\"unable to read config file\")\n",
    "\n",
    "model_config = config['train']['model']\n",
    "data_config = config['train']['data']\n",
    "train_params = config['train']['params']\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "os.makedirs(\"checkpoints4\", exist_ok=True)\n",
    "\n",
    "#writer = SummaryWriter()\n",
    "\n",
    "# Get data configuration\n",
    "\n",
    "train_labels = data_config[\"train_labels\"]\n",
    "valid_labels = data_config[\"valid_labels\"]\n",
    "img_path = data_config[\"img_path\"]\n",
    "#class_names = load_classes(data_config[\"class_names\"])\n",
    "\n",
    "# Initiate model\n",
    "model = Darknet(model_config[\"model_def\"]).to(device)\n",
    "model.apply(weights_init_normal)\n",
    "\n",
    "# If specified we start from checkpoint\n",
    "if model_config['pretrained_weights']:\n",
    "    if model_config['pretrained_weights'].endswith(\".pth\"):\n",
    "        model.load_state_dict(torch.load(model_config[\"pretrained_weights\"]))\n",
    "    else:\n",
    "        model.load_darknet_weights(model_config[\"pretrained_weights\"])\n",
    "\n",
    "# Get dataloader\n",
    "dataset = FaceTrainSet(img_path, train_labels)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, train_params[\"batch_size\"], shuffle=True,num_workers=train_params[\"n_cpu\"] ,\n",
    "            collate_fn = dataset.collate_fn)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "metrics = [\n",
    "    \"grid_size\",\n",
    "    \"loss\",\n",
    "    \"x\",\n",
    "    \"y\",\n",
    "    \"w\",\n",
    "    \"h\",\n",
    "    \"conf\",\n",
    "    \"conf_obj\",\n",
    "    \"conf_noobj\",\n",
    "]\n",
    "\n",
    "for epoch in range(train_params[\"epochs\"]):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for batch_i, (imgs, targets, _) in enumerate(dataloader):\n",
    "        batches_done = len(dataloader) * epoch + batch_i\n",
    "\n",
    "        imgs = Variable(imgs.to(device))\n",
    "        targets = Variable(targets.to(device), requires_grad=False)\n",
    "\n",
    "        loss, outputs = model(imgs, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        if batches_done % train_params[\"gradient_accumulations\"]:\n",
    "            # Accumulates gradient before each step\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # ----------------\n",
    "        #   Log progress\n",
    "        # ----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FaceTrainSet(img_path, train_labels)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, train_params[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_supression(predictions, conf_thres=0.5, nms_thres=0.4):\n",
    "    output = [None for _ in range(len(predictions))]\n",
    "    for i , img_pred in enumerate(predictions):\n",
    "        img_pred = img_pred[img_pred[:, 4] > conf_thres]\n",
    "        img_pred[:, :4] = x1y1x2y2(img_pred[:, :4])\n",
    "        if not img_pred.shape[0]:\n",
    "            continue\n",
    "        keep_boxes = []\n",
    "        while img_pred.size(0):\n",
    "            overlap_iou = bbox_iou(img_pred[0, :4].unsqueeze(0), img_pred[:, :4])\n",
    "            \n",
    "            invalid_iou = overlap_iou < nms_thres\n",
    "            valid_boxes= img_pred[~invalid_iou]\n",
    "            if valid_boxes.size(0):\n",
    "                weights = valid_boxes[:, 4:5]\n",
    "                box = torch.sum(valid_boxes[:, :4] * weights, axis = 0)/torch.sum(weights)\n",
    "                keep_boxes.append(box)\n",
    "            img_pred = img_pred[invalid_iou]\n",
    "        \n",
    "        output[i] = keep_boxes\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = torch.randint(1,10,(2, 49*3, 5)).float()\n",
    "#arr1 = arr[0][arr[0][:, 4] > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x1y1x2y2(boxes):\n",
    "    x1, x2 = boxes[:,0] - boxes[:,2]/2, boxes[:,0] + boxes[:,2]/2\n",
    "    y1, y2 = boxes[:,1] - boxes[:,3]/2, boxes[:,1]+boxes[:,3]/2\n",
    "    return torch.stack((x1,y1,x2,y2), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    a = 2\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Yolo_face_detection.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
